# =====================================================
# Certi-Hub ìë™ í¬ë¡¤ë§ ì›Œí¬í”Œë¡œìš°
# guide.md 4.3ì ˆ: ìë™ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸ (Automation)
# ë§¤ì¼ ì˜¤ì „ 9ì‹œ(KST) ì‹œí—˜ ì¼ì • í¬ë¡¤ë§
# =====================================================

name: ğŸ“… ìê²©ì¦ ì‹œí—˜ ì¼ì • ìë™ í¬ë¡¤ë§

on:
  schedule:
    # ë§¤ì¼ KST ì˜¤ì „ 9ì‹œ (UTC 00:00)
    - cron: "0 0 * * *"
  workflow_dispatch: # ìˆ˜ë™ ì‹¤í–‰ ì§€ì›

env:
  PYTHON_VERSION: "3.12"

jobs:
  crawl:
    name: ì‹œí—˜ ì¼ì • í¬ë¡¤ë§
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ ì½”ë“œ ì²´í¬ì•„ì›ƒ
        uses: actions/checkout@v4

      - name: ğŸ Python ì„¤ì •
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: pip
          cache-dependency-path: backend/requirements.txt

      - name: ğŸ“¦ ì˜ì¡´ì„± ì„¤ì¹˜
        run: |
          pip install -r backend/requirements.txt
          pip install scrapy selenium webdriver-manager

      - name: ğŸ•·ï¸ Q-Net í¬ë¡¤ëŸ¬ ì‹¤í–‰ (êµ­ê°€ê¸°ìˆ ìê²©)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: python -m crawlers.qnet_scraper
        working-directory: backend
        continue-on-error: true

      - name: ğŸ•·ï¸ KData í¬ë¡¤ëŸ¬ ì‹¤í–‰ (ë°ì´í„° ìê²©)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: python -m crawlers.kdata_scraper
        working-directory: backend
        continue-on-error: true

      - name: â˜ï¸ Cloud Vendor í¬ë¡¤ëŸ¬ ì‹¤í–‰ (AWS/GCP/Azure)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: python -m crawlers.cloud_scraper
        working-directory: backend
        continue-on-error: true

      - name: ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½
        if: always()
        run: |
          echo "========================================="
          echo "  í¬ë¡¤ë§ ì™„ë£Œ: $(date '+%Y-%m-%d %H:%M KST')"
          echo "========================================="
