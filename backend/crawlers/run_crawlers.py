"""
í¬ë¡¤ëŸ¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (guide.md 4.3 ìë™ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸)
ëª¨ë“  í¬ë¡¤ëŸ¬ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.

3ë‹¨ê³„ Fallback ì „ëµ (ê° í¬ë¡¤ëŸ¬ ê³µí†µ):
  1ë‹¨ê³„: ê³µì‹ API í˜¸ì¶œ
  2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§
  3ë‹¨ê³„: ìºì‹œ ë°ì´í„°

ì‚¬ìš©ë²•:
  python -m crawlers.run_crawlers              # ì „ì²´ ì‹¤í–‰
  python -m crawlers.run_crawlers --qnet       # Q-Netë§Œ
  python -m crawlers.run_crawlers --kdata      # KDataë§Œ
  python -m crawlers.run_crawlers --cloud      # Cloudë§Œ
  python -m crawlers.run_crawlers --finance    # ê¸ˆìœµ ìê²©ì¦ë§Œ
  python -m crawlers.run_crawlers --itdomestic # êµ­ë‚´ IT ìê²©ì¦ë§Œ
  python -m crawlers.run_crawlers --intl       # êµ­ì œ CBT ìê²©ì¦ë§Œ
"""

import sys
import time
import logging
import argparse
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(name)s] %(levelname)s: %(message)s",
)
logger = logging.getLogger("crawler_runner")


def run_qnet():
    """Q-Net í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    from crawlers.qnet_scraper import QNetScraper

    logger.info("=" * 60)
    logger.info("ğŸ•·ï¸  Q-Net í¬ë¡¤ëŸ¬ ì‹œì‘ (êµ­ê°€ê¸°ìˆ ìê²©)")
    logger.info("   ì „ëµ: API(ê³µê³µë°ì´í„°í¬í„¸) â†’ ì›¹í¬ë¡¤ë§(q-net.or.kr) â†’ ìºì‹œ")
    logger.info("=" * 60)
    start = time.time()
    scraper = QNetScraper()
    try:
        stats = scraper.save_to_db()
        elapsed = time.time() - start
        method = scraper.method_used
        logger.info(f"Q-Net ì™„ë£Œ: {elapsed:.1f}ì´ˆ, ìˆ˜ì§‘ë°©ë²•: {method}")
        return {"name": "Q-Net", "status": "success", "stats": stats, "time": elapsed, "method": method}
    except Exception as e:
        elapsed = time.time() - start
        logger.error(f"Q-Net í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {e}")
        return {"name": "Q-Net", "status": "failed", "error": str(e), "time": elapsed, "method": "failed"}
    finally:
        scraper.close()


def run_kdata():
    """KData í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    from crawlers.kdata_scraper import KDataScraper

    logger.info("=" * 60)
    logger.info("ğŸ•·ï¸  KData í¬ë¡¤ëŸ¬ ì‹œì‘ (ë°ì´í„° ìê²©ì‹œí—˜)")
    logger.info("   ì „ëµ: API(dataq.or.kr) â†’ ì›¹í¬ë¡¤ë§ â†’ ìºì‹œ")
    logger.info("=" * 60)
    start = time.time()
    scraper = KDataScraper()
    try:
        stats = scraper.save_to_db()
        elapsed = time.time() - start
        method = scraper.method_used
        logger.info(f"KData ì™„ë£Œ: {elapsed:.1f}ì´ˆ, ìˆ˜ì§‘ë°©ë²•: {method}")
        return {"name": "KData", "status": "success", "stats": stats, "time": elapsed, "method": method}
    except Exception as e:
        elapsed = time.time() - start
        logger.error(f"KData í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {e}")
        return {"name": "KData", "status": "failed", "error": str(e), "time": elapsed, "method": "failed"}
    finally:
        scraper.close()


def run_cloud():
    """Cloud Vendor í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    from crawlers.cloud_scraper import CloudScraper

    logger.info("=" * 60)
    logger.info("â˜ï¸  Cloud Vendor í¬ë¡¤ëŸ¬ ì‹œì‘ (AWS/GCP/Azure)")
    logger.info("   ì „ëµ: ë²¤ë”API(AWS/Azure) â†’ URLìœ íš¨ì„±í¬ë¡¤ë§ â†’ ìºì‹œ")
    logger.info("=" * 60)
    start = time.time()
    scraper = CloudScraper()
    try:
        stats = scraper.save_to_db()
        elapsed = time.time() - start
        method = scraper.method_used
        logger.info(f"Cloud ì™„ë£Œ: {elapsed:.1f}ì´ˆ, ìˆ˜ì§‘ë°©ë²•: {method}")
        return {"name": "Cloud", "status": "success", "stats": stats, "time": elapsed, "method": method}
    except Exception as e:
        elapsed = time.time() - start
        logger.error(f"Cloud í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {e}")
        return {"name": "Cloud", "status": "failed", "error": str(e), "time": elapsed, "method": "failed"}
    finally:
        scraper.close()


def run_finance():
    """ê¸ˆìœµ ìê²©ì¦ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    from crawlers.finance_scraper import FinanceScraper

    logger.info("=" * 60)
    logger.info("ğŸ’° ê¸ˆìœµ ìê²©ì¦ í¬ë¡¤ëŸ¬ ì‹œì‘ (KOFIA/KBI/FPKOREA)")
    logger.info("   ì „ëµ: AJAX API â†’ ì›¹í¬ë¡¤ë§ â†’ ìºì‹œ")
    logger.info("=" * 60)
    start = time.time()
    scraper = FinanceScraper()
    try:
        stats = scraper.save_to_db()
        elapsed = time.time() - start
        method = scraper.method_used
        logger.info(f"Finance ì™„ë£Œ: {elapsed:.1f}ì´ˆ, ìˆ˜ì§‘ë°©ë²•: {method}")
        return {"name": "Finance", "status": "success", "stats": stats, "time": elapsed, "method": method}
    except Exception as e:
        elapsed = time.time() - start
        logger.error(f"Finance í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {e}")
        return {"name": "Finance", "status": "failed", "error": str(e), "time": elapsed, "method": "failed"}
    finally:
        scraper.close()


def run_it_domestic():
    """êµ­ë‚´ IT ìê²©ì¦ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    from crawlers.it_domestic_scraper import ITDomesticScraper

    logger.info("=" * 60)
    logger.info("ğŸ–¥ï¸  êµ­ë‚´ IT ìê²©ì¦ í¬ë¡¤ëŸ¬ ì‹œì‘ (ICQA/IHD/KSTQB/ìƒê³µíšŒì˜ì†Œ)")
    logger.info("   ì „ëµ: ê¸°ê´€ API/ì›¹ â†’ í¬ë¡¤ë§ â†’ ìºì‹œ")
    logger.info("=" * 60)
    start = time.time()
    scraper = ITDomesticScraper()
    try:
        stats = scraper.save_to_db()
        elapsed = time.time() - start
        method = scraper.method_used
        logger.info(f"IT Domestic ì™„ë£Œ: {elapsed:.1f}ì´ˆ, ìˆ˜ì§‘ë°©ë²•: {method}")
        return {"name": "IT Domestic", "status": "success", "stats": stats, "time": elapsed, "method": method}
    except Exception as e:
        elapsed = time.time() - start
        logger.error(f"IT Domestic í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {e}")
        return {"name": "IT Domestic", "status": "failed", "error": str(e), "time": elapsed, "method": "failed"}
    finally:
        scraper.close()


def run_intl_cert():
    """êµ­ì œ CBT ìê²©ì¦ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    from crawlers.intl_cert_scraper import IntlCertScraper

    logger.info("=" * 60)
    logger.info("ğŸŒ êµ­ì œ CBT ìê²©ì¦ í¬ë¡¤ëŸ¬ ì‹œì‘ (ISC2/Cisco/Oracle/PMI...)")
    logger.info("   ì „ëµ: ë²¤ë”API â†’ URLìœ íš¨ì„±í™•ì¸ â†’ ìºì‹œ")
    logger.info("=" * 60)
    start = time.time()
    scraper = IntlCertScraper()
    try:
        stats = scraper.save_to_db()
        elapsed = time.time() - start
        method = scraper.method_used
        logger.info(f"Intl Cert ì™„ë£Œ: {elapsed:.1f}ì´ˆ, ìˆ˜ì§‘ë°©ë²•: {method}")
        return {"name": "Intl Cert", "status": "success", "stats": stats, "time": elapsed, "method": method}
    except Exception as e:
        elapsed = time.time() - start
        logger.error(f"Intl Cert í¬ë¡¤ëŸ¬ ì‹¤íŒ¨: {e}")
        return {"name": "Intl Cert", "status": "failed", "error": str(e), "time": elapsed, "method": "failed"}
    finally:
        scraper.close()


def print_summary(results):
    """ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    logger.info("")
    logger.info("=" * 60)
    logger.info(f"ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)

    total_inserted = 0
    total_updated = 0
    total_skipped = 0

    METHOD_LABELS = {
        "api": "ğŸŸ¢ ê³µì‹ API",
        "scraping": "ğŸŸ¡ ì›¹ í¬ë¡¤ë§",
        "cache": "ğŸŸ  ìºì‹œ ë°ì´í„°",
        "failed": "ğŸ”´ ì‹¤íŒ¨",
        "none": "âšª ë¯¸ì‹¤í–‰",
    }

    for r in results:
        status_icon = "âœ…" if r["status"] == "success" else "âŒ"
        method = r.get("method", "none")
        method_label = METHOD_LABELS.get(method, method)
        logger.info(f"  {status_icon} {r['name']}: {r['status']} ({r['time']:.1f}s) â€” {method_label}")

        if r["status"] == "success" and "stats" in r:
            stats = r["stats"]
            inserted = stats.get("inserted", 0)
            updated = stats.get("updated", 0)
            skipped = stats.get("skipped", 0)
            found = stats.get("found", 0)
            total_inserted += inserted
            total_updated += updated
            total_skipped += skipped
            logger.info(f"       ë§¤ì¹­: {found}, ì‹ ê·œ: {inserted}, ì—…ë°ì´íŠ¸: {updated}, ê±´ë„ˆëœ€: {skipped}")
        elif r["status"] == "failed":
            logger.info(f"       ì—ëŸ¬: {r.get('error', 'unknown')}")

    logger.info("-" * 60)
    logger.info(f"  ğŸ“ˆ í•©ê³„ â€” ì‹ ê·œ: {total_inserted}, ì—…ë°ì´íŠ¸: {total_updated}, ê±´ë„ˆëœ€: {total_skipped}")
    logger.info("=" * 60)

    return results


def run_all_crawlers() -> list:
    """
    ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰ (FastAPI ì—”ë“œí¬ì¸íŠ¸ìš© ë™ê¸° í•¨ìˆ˜)
    """
    results = [
        run_qnet(),
        run_kdata(),
        run_cloud(),
        run_finance(),
        run_it_domestic(),
        run_intl_cert(),
    ]
    print_summary(results)
    return results


def main():
    parser = argparse.ArgumentParser(description="Certi-Hub í¬ë¡¤ëŸ¬ ì‹¤í–‰ (3ë‹¨ê³„ Fallback)")
    parser.add_argument("--qnet", action="store_true", help="Q-Net í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰")
    parser.add_argument("--kdata", action="store_true", help="KData í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰")
    parser.add_argument("--cloud", action="store_true", help="Cloud í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰")
    parser.add_argument("--finance", action="store_true", help="ê¸ˆìœµ ìê²©ì¦ í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰")
    parser.add_argument("--itdomestic", action="store_true", help="êµ­ë‚´ IT ìê²©ì¦ í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰")
    parser.add_argument("--intl", action="store_true", help="êµ­ì œ CBT ìê²©ì¦ í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰")
    args = parser.parse_args()

    # ì•„ë¬´ ì˜µì…˜ë„ ì—†ìœ¼ë©´ ì „ì²´ ì‹¤í–‰
    run_all = not (args.qnet or args.kdata or args.cloud or args.finance or args.itdomestic or args.intl)

    results = []

    if run_all or args.qnet:
        results.append(run_qnet())

    if run_all or args.kdata:
        results.append(run_kdata())

    if run_all or args.cloud:
        results.append(run_cloud())

    if run_all or args.finance:
        results.append(run_finance())

    if run_all or args.itdomestic:
        results.append(run_it_domestic())

    if run_all or args.intl:
        results.append(run_intl_cert())

    print_summary(results)

    # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ exit code 1
    if any(r["status"] == "failed" for r in results):
        sys.exit(1)


if __name__ == "__main__":
    main()
