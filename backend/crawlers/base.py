"""
í¬ë¡¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤ + DB ì—…ì„œíŠ¸ + ìºì‹œ ìœ í‹¸ë¦¬í‹°
guide.md 4.3: Conflict Resolution - ê¸°ì¡´ ë°ì´í„°ì™€ ì¤‘ë³µ ì‹œ updated_atë§Œ ê°±ì‹ 

3ë‹¨ê³„ Fallback ì „ëµ:
  1ë‹¨ê³„: ê³µì‹ API í˜¸ì¶œ (ê°€ì¥ ì •í™•)
  2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§ / HTML íŒŒì‹±
  3ë‹¨ê³„: ìºì‹œ ë°ì´í„° (ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„°)
"""

import json
import logging
import os
from abc import ABC, abstractmethod
from datetime import datetime, date
from functools import lru_cache
from pathlib import Path
from typing import List, Dict, Optional

from sqlalchemy import create_engine, text
from sqlalchemy.orm import Session

logger = logging.getLogger("crawlers.base")

# ìºì‹œ ë””ë ‰í† ë¦¬
CACHE_DIR = Path(os.getenv("CACHE_DIR", "/app/cache"))


# ============================================================
# DB í—¬í¼
# ============================================================

@lru_cache(maxsize=1)
def get_sync_engine():
    """ë™ê¸° DB ì—”ì§„ (í¬ë¡¤ëŸ¬ìš©) â€” ì‹±ê¸€í„´ ìºì‹œ
    
    í™˜ê²½ë³€ìˆ˜ ìš°ì„ ìˆœìœ„:
    1. DATABASE_URL_SYNC í™˜ê²½ë³€ìˆ˜
    2. config.py Settings (pydantic-settings, .env íŒŒì¼ ë¡œë“œ)
    3. í•˜ë“œì½”ë”© ê¸°ë³¸ê°’
    """
    default_url = "postgresql+psycopg2://postgres:postgres@localhost:5432/certihub"
    
    # os.getenvëŠ” ë¹ˆ ë¬¸ìì—´ë„ ë°˜í™˜í•˜ë¯€ë¡œ `or`ë¡œ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
    url = os.getenv("DATABASE_URL_SYNC", "").strip() or None
    
    # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ config.py Settingsì—ì„œ ê°€ì ¸ì˜¤ê¸° (.env íŒŒì¼ ë¡œë“œë¨)
    if not url:
        try:
            from config import get_settings
            url = get_settings().DATABASE_URL_SYNC
        except Exception:
            pass
    
    url = url or default_url
    
    logger.info(f"ğŸ”— ë™ê¸° DB ì—°ê²°: {url.split('@')[-1] if '@' in url else '(default)'}")
    
    return create_engine(
        url,
        echo=False,
        pool_size=5,
        max_overflow=3,
        pool_recycle=1800,
        pool_pre_ping=True,
    )


def find_cert_id(session: Session, name_ko: str) -> Optional[str]:
    """ìê²©ì¦ ì´ë¦„(í•œê¸€)ìœ¼ë¡œ cert_id ì¡°íšŒ"""
    result = session.execute(
        text("SELECT id FROM certifications WHERE name_ko = :name"),
        {"name": name_ko},
    )
    row = result.fetchone()
    return str(row[0]) if row else None


def find_cert_id_like(session: Session, keyword: str) -> Optional[str]:
    """ìê²©ì¦ ì´ë¦„ ë¶€ë¶„ì¼ì¹˜ë¡œ cert_id ì¡°íšŒ"""
    result = session.execute(
        text("SELECT id FROM certifications WHERE name_ko ILIKE :kw OR name_en ILIKE :kw LIMIT 1"),
        {"kw": f"%{keyword}%"},
    )
    row = result.fetchone()
    return str(row[0]) if row else None


def upsert_schedule(
    session: Session,
    cert_id: str,
    round_no: int,
    reg_start: Optional[date],
    reg_end: Optional[date],
    exam_date: Optional[date],
    result_date: Optional[date],
) -> str:
    """
    ì‹œí—˜ ì¼ì • ì—…ì„œíŠ¸ (guide.md 4.3 Conflict Resolution)
    - cert_id + round ì¡°í•©ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° í™•ì¸
    - ê¸°ì¡´ ë°ì´í„° ìˆìœ¼ë©´ updated_atë§Œ ê°±ì‹ 
    - ì—†ìœ¼ë©´ ìƒˆë¡œ INSERT
    """
    existing = session.execute(
        text("SELECT id FROM exam_schedules WHERE cert_id = :cid AND round = :r"),
        {"cid": cert_id, "r": round_no},
    ).fetchone()

    if existing:
        session.execute(
            text("""
                UPDATE exam_schedules
                SET reg_start = COALESCE(:rs, reg_start),
                    reg_end = COALESCE(:re, reg_end),
                    exam_date = COALESCE(:ed, exam_date),
                    result_date = COALESCE(:rd, result_date),
                    updated_at = NOW()
                WHERE cert_id = :cid AND round = :r
            """),
            {
                "rs": reg_start,
                "re": reg_end,
                "ed": exam_date,
                "rd": result_date,
                "cid": cert_id,
                "r": round_no,
            },
        )
        return "updated"
    else:
        session.execute(
            text("""
                INSERT INTO exam_schedules (cert_id, round, reg_start, reg_end, exam_date, result_date)
                VALUES (:cid, :r, :rs, :re, :ed, :rd)
            """),
            {
                "cid": cert_id,
                "r": round_no,
                "rs": reg_start,
                "re": reg_end,
                "ed": exam_date,
                "rd": result_date,
            },
        )
        return "inserted"


# ============================================================
# ë‚ ì§œ íŒŒì‹±
# ============================================================

def parse_date(date_str: str) -> Optional[date]:
    """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±"""
    if not date_str or not date_str.strip():
        return None

    date_str = date_str.strip().replace(".", "-").replace("/", "-")

    for fmt in ["%Y-%m-%d", "%Y-%m-%d(%a)", "%Y-%m-%d(%A)", "%m-%d"]:
        try:
            parsed = datetime.strptime(date_str, fmt)
            if fmt == "%m-%d":
                parsed = parsed.replace(year=datetime.now().year)
            return parsed.date()
        except ValueError:
            continue

    # ìˆ«ìë§Œ ì¶”ì¶œ ì‹œë„ (20260315 í˜•íƒœ)
    digits = "".join(c for c in date_str if c.isdigit())
    if len(digits) == 8:
        try:
            return datetime.strptime(digits, "%Y%m%d").date()
        except ValueError:
            pass

    return None


# ============================================================
# ìºì‹œ ìœ í‹¸ë¦¬í‹°
# ============================================================

def save_cache(source: str, data: List[Dict]):
    """
    ìˆ˜ì§‘ ì„±ê³µí•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ìºì‹±
    ë‹¤ìŒì— API + í¬ë¡¤ë§ ëª¨ë‘ ì‹¤íŒ¨í•´ë„ ì´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ
    """
    try:
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
        cache_file = CACHE_DIR / f"{source}_schedules.json"
        payload = {
            "fetched_at": datetime.now().isoformat(),
            "source": source,
            "count": len(data),
            "schedules": data,
        }
        cache_file.write_text(json.dumps(payload, ensure_ascii=False, default=str))
        logging.getLogger(source).info(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_file} ({len(data)}ê±´)")
    except Exception as e:
        logging.getLogger(source).warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


def load_cache(source: str) -> List[Dict]:
    """
    ë§ˆì§€ë§‰ìœ¼ë¡œ ì„±ê³µí•œ ìºì‹œ ë°ì´í„° ë¡œë“œ
    APIì™€ í¬ë¡¤ë§ ëª¨ë‘ ì‹¤íŒ¨í–ˆì„ ë•Œ ì‚¬ìš©
    """
    logger = logging.getLogger(source)
    cache_file = CACHE_DIR / f"{source}_schedules.json"

    if not cache_file.exists():
        logger.info(f"ìºì‹œ íŒŒì¼ ì—†ìŒ: {cache_file}")
        return []

    try:
        payload = json.loads(cache_file.read_text())
        schedules = payload.get("schedules", [])
        fetched_at = payload.get("fetched_at", "unknown")
        logger.info(f"ğŸ“‚ ìºì‹œ ë¡œë“œ: {len(schedules)}ê±´ (ìˆ˜ì§‘ì¼: {fetched_at})")
        return schedules
    except Exception as e:
        logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []


# ============================================================
# 3ë‹¨ê³„ Fallback ë² ì´ìŠ¤ í´ë˜ìŠ¤
# ============================================================

class BaseScraper(ABC):
    """
    ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤
    3ë‹¨ê³„ Fallback ì „ëµì„ ê°•ì œí•©ë‹ˆë‹¤:
      1ë‹¨ê³„: try_official_api()  - ê³µì‹ API í˜¸ì¶œ
      2ë‹¨ê³„: try_web_scraping()  - ì›¹ í¬ë¡¤ë§
      3ë‹¨ê³„: load_cache()        - ìºì‹œ ë°ì´í„°
    """

    source_name: str = "base"

    def __init__(self):
        self.logger = logging.getLogger(self.source_name)
        self.stats = {"found": 0, "inserted": 0, "updated": 0, "skipped": 0}
        self.method_used = "none"  # ì–´ë–¤ ë‹¨ê³„ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ëŠ”ì§€ ê¸°ë¡

    def fetch_schedules(self) -> List[Dict]:
        """
        3ë‹¨ê³„ Fallbackìœ¼ë¡œ ì‹œí—˜ ì¼ì • ìˆ˜ì§‘

        Returns:
            ìˆ˜ì§‘ëœ ì¼ì • ëª©ë¡ (ì–´ë–¤ ë‹¨ê³„ì—ì„œë“  ì„±ê³µí•˜ë©´ ë°˜í™˜)
        """

        # === 1ë‹¨ê³„: ê³µì‹ API ===
        self.logger.info("ğŸ“¡ [1ë‹¨ê³„] ê³µì‹ API í˜¸ì¶œ ì‹œë„...")
        schedules = self.try_official_api()
        if schedules:
            self.method_used = "api"
            self.logger.info(f"âœ… [1ë‹¨ê³„ ì„±ê³µ] APIì—ì„œ {len(schedules)}ê±´ ìˆ˜ì§‘")
            save_cache(self.source_name, schedules)
            return schedules
        self.logger.info("âš ï¸  [1ë‹¨ê³„ ì‹¤íŒ¨] APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•¨")

        # === 2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§ ===
        self.logger.info("ğŸ•·ï¸  [2ë‹¨ê³„] ì›¹ í¬ë¡¤ë§ ì‹œë„...")
        schedules = self.try_web_scraping()
        if schedules:
            self.method_used = "scraping"
            self.logger.info(f"âœ… [2ë‹¨ê³„ ì„±ê³µ] í¬ë¡¤ë§ì—ì„œ {len(schedules)}ê±´ ìˆ˜ì§‘")
            save_cache(self.source_name, schedules)
            return schedules
        self.logger.info("âš ï¸  [2ë‹¨ê³„ ì‹¤íŒ¨] í¬ë¡¤ë§ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•¨")

        # === 3ë‹¨ê³„: ìºì‹œ ===
        self.logger.info("ğŸ“‚ [3ë‹¨ê³„] ìºì‹œ ë°ì´í„° ë¡œë“œ ì‹œë„...")
        schedules = load_cache(self.source_name)
        if schedules:
            self.method_used = "cache"
            self.logger.info(f"âœ… [3ë‹¨ê³„ ì„±ê³µ] ìºì‹œì—ì„œ {len(schedules)}ê±´ ë¡œë“œ")
            return schedules

        self.logger.error("âŒ ëª¨ë“  ìˆ˜ì§‘ ë°©ë²• ì‹¤íŒ¨ â€” ë°ì´í„° ì—†ìŒ")
        self.method_used = "failed"
        return []

    @abstractmethod
    def try_official_api(self) -> List[Dict]:
        """1ë‹¨ê³„: ê³µì‹ API í˜¸ì¶œ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        ...

    @abstractmethod
    def try_web_scraping(self) -> List[Dict]:
        """2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§ (ì„œë¸Œí´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)"""
        ...

    def save_to_db(self) -> Dict:
        """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
        engine = get_sync_engine()
        schedules = self.fetch_schedules()

        if not schedules:
            self.logger.warning("ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
            return self.stats

        with Session(engine) as session:
            for sch in schedules:
                cert_name = sch.get("cert_name", "")
                if not cert_name:
                    continue

                cert_id = find_cert_id(session, cert_name)
                if not cert_id:
                    cert_id = find_cert_id_like(session, cert_name)

                if not cert_id:
                    self.logger.warning(f"DBì—ì„œ '{cert_name}' ìê²©ì¦ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ â†’ ê±´ë„ˆëœ€")
                    self.stats["skipped"] += 1
                    continue

                self.stats["found"] += 1
                result = upsert_schedule(
                    session=session,
                    cert_id=cert_id,
                    round_no=sch.get("round", 1),
                    reg_start=parse_date(str(sch.get("reg_start", ""))),
                    reg_end=parse_date(str(sch.get("reg_end", ""))),
                    exam_date=parse_date(str(sch.get("exam_date", ""))),
                    result_date=parse_date(str(sch.get("result_date", ""))),
                )
                self.stats[result] = self.stats.get(result, 0) + 1

            session.commit()

        self.logger.info(
            f"ğŸ“Š {self.source_name} ì™„ë£Œ [ë°©ë²•: {self.method_used}]: "
            f"ë§¤ì¹­ {self.stats['found']}ê±´, "
            f"ì‹ ê·œ {self.stats['inserted']}ê±´, "
            f"ì—…ë°ì´íŠ¸ {self.stats['updated']}ê±´, "
            f"ê±´ë„ˆëœ€ {self.stats['skipped']}ê±´"
        )
        return self.stats

    @abstractmethod
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        ...
