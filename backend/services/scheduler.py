"""
APScheduler ê¸°ë°˜ ì •ê¸° í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬
ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ì „ì²´ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ê³  seed-events.tsë¥¼ ë™ê¸°í™”í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  - FastAPI lifespanì—ì„œ start_scheduler() / stop_scheduler() í˜¸ì¶œ
  - /api/crawl/trigger ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥
"""

import logging
from datetime import datetime, timezone
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

logger = logging.getLogger("scheduler")

# ì‹±ê¸€í„´ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
_scheduler: AsyncIOScheduler | None = None


def get_scheduler() -> AsyncIOScheduler | None:
    return _scheduler


async def run_crawl_job(source: str = "all"):
    """
    í¬ë¡¤ë§ ì‹¤í–‰ + CrawlLog ê¸°ë¡ + seed-events.ts ë™ê¸°í™”
    APScheduler ì¡ ë˜ëŠ” ìˆ˜ë™ íŠ¸ë¦¬ê±°ì—ì„œ í˜¸ì¶œ
    """
    import asyncio
    import time

    from sqlalchemy.orm import Session
    from crawlers.base import get_sync_engine
    from models import CrawlLog

    engine = get_sync_engine()

    # ì‹¤í–‰í•  í¬ë¡¤ëŸ¬ ë§¤í•‘
    crawler_map = {
        "qnet": ("crawlers.qnet_scraper", "QNetScraper", "Q-Net"),
        "kdata": ("crawlers.kdata_scraper", "KDataScraper", "KData"),
        "cloud": ("crawlers.cloud_scraper", "CloudScraper", "Cloud"),
        "finance": ("crawlers.finance_scraper", "FinanceScraper", "Finance"),
        "it_domestic": ("crawlers.it_domestic_scraper", "ITDomesticScraper", "IT Domestic"),
        "intl": ("crawlers.intl_cert_scraper", "IntlCertScraper", "Intl Cert"),
    }

    sources_to_run = list(crawler_map.keys()) if source == "all" else [source]
    results = []

    loop = asyncio.get_running_loop()

    for src in sources_to_run:
        if src not in crawler_map:
            continue

        module_path, class_name, display_name = crawler_map[src]

        # CrawlLog ì‹œì‘ ê¸°ë¡
        with Session(engine) as session:
            log = CrawlLog(source=src, status="running", started_at=datetime.now(timezone.utc))
            session.add(log)
            session.commit()
            log_id = log.id

        start_time = time.time()

        try:
            # í¬ë¡¤ëŸ¬ ë™ì  ì„í¬íŠ¸ ë° ì‹¤í–‰
            def _run_scraper():
                import importlib
                mod = importlib.import_module(module_path)
                scraper_cls = getattr(mod, class_name)
                scraper = scraper_cls()
                try:
                    stats = scraper.save_to_db()
                    method = scraper.method_used
                    return {"stats": stats, "method": method, "error": None}
                finally:
                    scraper.close()

            result = await loop.run_in_executor(None, _run_scraper)
            elapsed = time.time() - start_time

            # CrawlLog ì„±ê³µ ê¸°ë¡
            with Session(engine) as session:
                log = session.get(CrawlLog, log_id)
                if log:
                    log.status = "success"
                    log.method = result["method"]
                    log.found = result["stats"].get("found", 0)
                    log.inserted = result["stats"].get("inserted", 0)
                    log.updated = result["stats"].get("updated", 0)
                    log.skipped = result["stats"].get("skipped", 0)
                    log.duration_sec = round(elapsed, 2)
                    log.finished_at = datetime.now(timezone.utc)
                    log.detail = result["stats"]
                    session.commit()

            results.append({
                "source": src,
                "name": display_name,
                "status": "success",
                "method": result["method"],
                "stats": result["stats"],
                "time": round(elapsed, 2),
            })
            logger.info(f"âœ… {display_name} í¬ë¡¤ë§ ì™„ë£Œ: {result['method']} ({elapsed:.1f}s)")

        except Exception as e:
            elapsed = time.time() - start_time
            error_msg = str(e)

            # CrawlLog ì‹¤íŒ¨ ê¸°ë¡
            with Session(engine) as session:
                log = session.get(CrawlLog, log_id)
                if log:
                    log.status = "failed"
                    log.error_message = error_msg[:1000]
                    log.duration_sec = round(elapsed, 2)
                    log.finished_at = datetime.now(timezone.utc)
                    session.commit()

            results.append({
                "source": src,
                "name": display_name,
                "status": "failed",
                "error": error_msg,
                "time": round(elapsed, 2),
            })
            logger.error(f"âŒ {display_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    # í¬ë¡¤ë§ ì™„ë£Œ í›„ seed-events.ts ë™ê¸°í™”
    try:
        from services.seed_sync import sync_seed_events
        sync_result = sync_seed_events()
        logger.info(f"ğŸ“ seed-events.ts ë™ê¸°í™” ì™„ë£Œ: {sync_result['events_count']}ê±´")
    except Exception as e:
        logger.warning(f"âš ï¸ seed-events.ts ë™ê¸°í™” ì‹¤íŒ¨ (ì„œë¹„ìŠ¤ ìš´ì˜ì— ì˜í–¥ ì—†ìŒ): {e}")

    return results


async def _scheduled_crawl_job():
    """ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ í˜¸ì¶œë˜ëŠ” ë˜í¼"""
    logger.info("â° ì •ê¸° í¬ë¡¤ë§ ì‹œì‘ (APScheduler)")
    try:
        results = await run_crawl_job("all")
        success = sum(1 for r in results if r["status"] == "success")
        failed = sum(1 for r in results if r["status"] == "failed")
        logger.info(f"â° ì •ê¸° í¬ë¡¤ë§ ì™„ë£Œ: ì„±ê³µ {success}ê±´, ì‹¤íŒ¨ {failed}ê±´")
    except Exception as e:
        logger.error(f"â° ì •ê¸° í¬ë¡¤ë§ ì—ëŸ¬: {e}")


def start_scheduler():
    """
    APScheduler ì‹œì‘ (FastAPI lifespanì—ì„œ í˜¸ì¶œ)
    ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
    """
    global _scheduler

    _scheduler = AsyncIOScheduler(timezone="Asia/Seoul")

    # ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ì „ì²´ í¬ë¡¤ë§
    _scheduler.add_job(
        _scheduled_crawl_job,
        trigger=CronTrigger(hour=3, minute=0, timezone="Asia/Seoul"),
        id="daily_crawl",
        name="ë§¤ì¼ ìƒˆë²½ 3ì‹œ ì „ì²´ í¬ë¡¤ë§",
        replace_existing=True,
    )

    _scheduler.start()
    logger.info("ğŸ• APScheduler ì‹œì‘ â€” ë§¤ì¼ 03:00 KST í¬ë¡¤ë§ ì˜ˆì•½ë¨")


def stop_scheduler():
    """APScheduler ì¢…ë£Œ"""
    global _scheduler
    if _scheduler and _scheduler.running:
        _scheduler.shutdown(wait=False)
        logger.info("ğŸ• APScheduler ì¢…ë£Œ")
    _scheduler = None
