# ğŸ•·ï¸ í¬ë¡¤ëŸ¬ ì‹œìŠ¤í…œ (Crawler System)

> Certi-Hub í¬ë¡¤ëŸ¬ì˜ 3ë‹¨ê³„ Fallback ì „ëµ, í¬ë¡¤ëŸ¬ ì¢…ë¥˜, ì‹¤í–‰ ë°©ë²•, ìš´ì˜ ê°€ì´ë“œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
>
> ğŸ“ ì†ŒìŠ¤ ìœ„ì¹˜: `backend/crawlers/`

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ê°œìš”

Certi-Hub í¬ë¡¤ëŸ¬ëŠ” ë‹¤ì–‘í•œ ì™¸ë¶€ ì†ŒìŠ¤(ê³µê³µë°ì´í„° API, ìê²©ì¦ ê¸°ê´€ ì›¹ì‚¬ì´íŠ¸)ì—ì„œ ì‹œí—˜ ì¼ì • ë° ìê²©ì¦ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤. **3ë‹¨ê³„ Fallback ì „ëµ**ì„ í†µí•´ ì–´ë–¤ ìƒí™©ì—ì„œë„ ìµœì‹  ë°ì´í„°ë¥¼ ì œê³µí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

```mermaid
flowchart TD
    Start([í¬ë¡¤ëŸ¬ ì‹œì‘]) --> Step1{1ë‹¨ê³„: ê³µì‹ API}
    Step1 -->|ì„±ê³µ| SaveCache[ìºì‹œ ì €ì¥]
    Step1 -->|ì‹¤íŒ¨| Step2{2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§}
    Step2 -->|ì„±ê³µ| SaveCache
    Step2 -->|ì‹¤íŒ¨| Step3{3ë‹¨ê³„: ìºì‹œ ë¡œë“œ}
    Step3 -->|ìºì‹œ ìˆìŒ| UseCache[ìºì‹œ ë°ì´í„° ì‚¬ìš©]
    Step3 -->|ìºì‹œ ì—†ìŒ| Fail[âŒ ìˆ˜ì§‘ ì‹¤íŒ¨]
    SaveCache --> SaveDB[DB ì €ì¥ Upsert]
    UseCache --> SaveDB
    SaveDB --> SyncSeed[seed-events.ts ë™ê¸°í™”]
    SyncSeed --> End([ì™„ë£Œ])
```

---

## ğŸ¯ 3ë‹¨ê³„ Fallback ì „ëµ

ëª¨ë“  í¬ë¡¤ëŸ¬ëŠ” `BaseScraper` í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ë©°, ë™ì¼í•œ 3ë‹¨ê³„ ì „ëµì„ ë”°ë¦…ë‹ˆë‹¤.

### 1ë‹¨ê³„: ê³µì‹ API í˜¸ì¶œ (`try_official_api`)

| í•­ëª© | ì„¤ëª… |
|------|------|
| **ë°©ë²•** | ê³µì‹ Open API (REST) í˜¸ì¶œ |
| **ì¥ì ** | ê°€ì¥ ì •í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„° |
| **ë‹¨ì ** | API Key í•„ìš”, API ìŠ¤í™ ë³€ê²½ ê°€ëŠ¥ì„± |
| **ì˜ˆì‹œ** | ê³µê³µë°ì´í„°í¬í„¸(data.go.kr) Q-Net API, AWS Certification API |

### 2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§ (`try_web_scraping`)

| í•­ëª© | ì„¤ëª… |
|------|------|
| **ë°©ë²•** | HTML í˜ì´ì§€ íŒŒì‹± (httpx + BeautifulSoup) |
| **ì¥ì ** | API ì—†ì´ë„ ë°ì´í„° ìˆ˜ì§‘ ê°€ëŠ¥ |
| **ë‹¨ì ** | ì›¹í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ì— ì·¨ì•½ |
| **ì˜ˆì‹œ** | Q-Net ì‹œí—˜ì¼ì • í˜ì´ì§€ í…Œì´ë¸” íŒŒì‹±, KData ì›¹í˜ì´ì§€ íŒŒì‹± |

### 3ë‹¨ê³„: ìºì‹œ ë°ì´í„° (`load_cache`)

| í•­ëª© | ì„¤ëª… |
|------|------|
| **ë°©ë²•** | ë§ˆì§€ë§‰ ì„±ê³µ ì‹œ ì €ì¥ëœ JSON ìºì‹œ íŒŒì¼ ë¡œë“œ |
| **ì¥ì ** | ì™¸ë¶€ ì„œë¹„ìŠ¤ ì¥ì•  ì‹œì—ë„ ì„œë¹„ìŠ¤ ì—°ì†ì„± ë³´ì¥ |
| **ë‹¨ì ** | ë°ì´í„°ê°€ êµ¬ë²„ì „ì¼ ìˆ˜ ìˆìŒ |
| **ì €ì¥ ìœ„ì¹˜** | `/app/cache/{source}_schedules.json` |

> ğŸ’¡ **í•µì‹¬ ì›ì¹™**: 1ë‹¨ê³„ ë˜ëŠ” 2ë‹¨ê³„ ì„±ê³µ ì‹œ ë°˜ë“œì‹œ ìºì‹œë¥¼ ê°±ì‹ í•˜ì—¬, ë‹¤ìŒ ë²ˆ 3ë‹¨ê³„ ì‹œ ìµœì‹  ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

---

## ğŸ•·ï¸ í¬ë¡¤ëŸ¬ ëª©ë¡

| í¬ë¡¤ëŸ¬ | í´ë˜ìŠ¤ | ì†ŒìŠ¤ ì´ë¦„ | ëŒ€ìƒ |
|--------|--------|-----------|------|
| **Q-Net** | `QNetScraper` | `qnet` | êµ­ê°€ê¸°ìˆ ìê²© (ì •ë³´ì²˜ë¦¬ê¸°ì‚¬, ì •ë³´ë³´ì•ˆê¸°ì‚¬ ë“±) |
| **KData** | `KDataScraper` | `kdata` | ë°ì´í„° ìê²©ì‹œí—˜ (SQLD, SQLP, DAP, DAsP) |
| **Cloud** | `CloudScraper` | `cloud` | í´ë¼ìš°ë“œ ë²¤ë” ìê²©ì¦ (AWS, GCP, Azure) |
| **Finance** | `FinanceScraper` | `finance` | ê¸ˆìœµ ìê²©ì¦ (KOFIA, KBI, FPKOREA) |
| **IT Domestic** | `ITDomesticScraper` | `it_domestic` | êµ­ë‚´ IT ìê²©ì¦ (ICQA, IHD, KSTQB ë“±) |
| **Intl Cert** | `IntlCertScraper` | `intl` | êµ­ì œ CBT ìê²©ì¦ (ISC2, Cisco, Oracle, PMI ë“±) |

---

## ğŸ“‹ í¬ë¡¤ëŸ¬ë³„ ìƒì„¸

### 1. Q-Net í¬ë¡¤ëŸ¬ (`qnet_scraper.py`)

êµ­ê°€ê¸°ìˆ ìê²© ì‹œí—˜ ì¼ì •ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

| ë‹¨ê³„ | ì†ŒìŠ¤ | ì„¤ëª… |
|------|------|------|
| 1ë‹¨ê³„ | ê³µê³µë°ì´í„°í¬í„¸ API | `data.go.kr` Q-Net ì‹œí—˜ì¼ì • API (`DATA_GO_KR_API_KEY` í•„ìš”) |
| 2ë‹¨ê³„ | Q-Net ì›¹í¬ë¡¤ë§ | `q-net.or.kr` ì‹œí—˜ì¼ì • í˜ì´ì§€ í…Œì´ë¸” íŒŒì‹± |
| 2-1 | Known ì¼ì • | ì›¹ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ì…ë ¥ëœ ê³µê°œ ì¼ì • ë°ì´í„° ì‚¬ìš© |
| 3ë‹¨ê³„ | ìºì‹œ | ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„° |

**í™˜ê²½ë³€ìˆ˜**: `DATA_GO_KR_API_KEY` â€” [ê³µê³µë°ì´í„°í¬í„¸](https://www.data.go.kr)ì—ì„œ ë¬´ë£Œ ë°œê¸‰

**ì£¼ìš” ëŒ€ìƒ ìê²©ì¦**:
- ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ / ì‚°ì—…ê¸°ì‚¬
- ì •ë³´ë³´ì•ˆê¸°ì‚¬ / ì‚°ì—…ê¸°ì‚¬
- ë¹…ë°ì´í„°ë¶„ì„ê¸°ì‚¬
- ì •ë³´í†µì‹ ê¸°ì‚¬
- ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ 1ê¸‰
- ì„œë¹„ìŠ¤ê²½í—˜ë””ìì¸ê¸°ì‚¬

---

### 2. KData í¬ë¡¤ëŸ¬ (`kdata_scraper.py`)

ë°ì´í„° ê´€ë ¨ ìê²©ì‹œí—˜ ì¼ì •ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

| ë‹¨ê³„ | ì†ŒìŠ¤ | ì„¤ëª… |
|------|------|------|
| 1ë‹¨ê³„ | KData API | `dataq.or.kr` ì‹œí—˜ì¼ì • API/AJAX |
| 2ë‹¨ê³„ | KData ì›¹í¬ë¡¤ë§ | ì›¹í˜ì´ì§€ íŒŒì‹± |
| 3ë‹¨ê³„ | ìºì‹œ | ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„° |

**ì£¼ìš” ëŒ€ìƒ**: SQLD, SQLP, DAP, DAsP, ADP

---

### 3. Cloud í¬ë¡¤ëŸ¬ (`cloud_scraper.py`)

í´ë¼ìš°ë“œ ë²¤ë” ìê²©ì¦ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

| ë‹¨ê³„ | ì†ŒìŠ¤ | ì„¤ëª… |
|------|------|------|
| 1ë‹¨ê³„ | ë²¤ë” ê³µì‹ API | AWS Certification API, Azure/MS Learn API |
| 2ë‹¨ê³„ | URL ìœ íš¨ì„± í™•ì¸ | ê° ìê²©ì¦ ê³µì‹ í˜ì´ì§€ HTTP HEAD ìš”ì²­ |
| 3ë‹¨ê³„ | ìºì‹œ | ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„° |

**íŠ¹ì´ì‚¬í•­**:
- í´ë¼ìš°ë“œ ìê²©ì¦ì€ **ìƒì‹œ ì ‘ìˆ˜** â†’ ì‹œí—˜ ì¼ì • INSERTê°€ ì•„ë‹Œ **URL + `updated_at` ê°±ì‹ **ì— ì´ˆì 
- ì»¤ìŠ¤í…€ `save_to_db()` ë©”ì„œë“œ ì‚¬ìš© (ë¶€ëª¨ í´ë˜ìŠ¤ì™€ ë‹¤ë¥¸ ë¡œì§)

**ëŒ€ìƒ ë²¤ë” ë° ìê²©ì¦**:
- **AWS**: CLF-C02, SAA-C03, DVA-C02, SAP-C02
- **GCP**: ACE, PCA, PDE, PCSE
- **Azure**: AZ-900, AZ-104, AZ-305, AZ-204

---

### 4. Finance í¬ë¡¤ëŸ¬ (`finance_scraper.py`)

ê¸ˆìœµ ë¶„ì•¼ ìê²©ì¦ ì¼ì •ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

| ë‹¨ê³„ | ì†ŒìŠ¤ | ì„¤ëª… |
|------|------|------|
| 1ë‹¨ê³„ | AJAX API | KOFIA/KBI/FPKOREA AJAX ìš”ì²­ |
| 2ë‹¨ê³„ | ì›¹í¬ë¡¤ë§ | ê° ê¸°ê´€ ì›¹í˜ì´ì§€ íŒŒì‹± |
| 3ë‹¨ê³„ | ìºì‹œ | ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„° |

**ì£¼ìš” ëŒ€ìƒ**: í€ë“œíˆ¬ìê¶Œìœ ìë¬¸ì¸ë ¥, íˆ¬ììì‚°ìš´ìš©ì‚¬, CFP, AFPK

---

### 5. IT Domestic í¬ë¡¤ëŸ¬ (`it_domestic_scraper.py`)

êµ­ë‚´ IT ìê²©ì¦ ì¼ì •ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

| ë‹¨ê³„ | ì†ŒìŠ¤ | ì„¤ëª… |
|------|------|------|
| 1ë‹¨ê³„ | ê¸°ê´€ API/ì›¹ | ICQA/IHD/KSTQB/ìƒê³µíšŒì˜ì†Œ |
| 2ë‹¨ê³„ | ì›¹í¬ë¡¤ë§ | ê° ê¸°ê´€ ì‹œí—˜ì¼ì • í˜ì´ì§€ |
| 3ë‹¨ê³„ | ìºì‹œ | ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„° |

**ì£¼ìš” ëŒ€ìƒ**: ë„¤íŠ¸ì›Œí¬ê´€ë¦¬ì‚¬, ë¦¬ëˆ…ìŠ¤ë§ˆìŠ¤í„°, ISTQB/KSTQB, ì „ììƒê±°ë˜ê´€ë¦¬ì‚¬

---

### 6. Intl Cert í¬ë¡¤ëŸ¬ (`intl_cert_scraper.py`)

êµ­ì œ CBT ìê²©ì¦ ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

| ë‹¨ê³„ | ì†ŒìŠ¤ | ì„¤ëª… |
|------|------|------|
| 1ë‹¨ê³„ | ë²¤ë” API | ISC2/Cisco/Oracle/PMI ê³µì‹ API |
| 2ë‹¨ê³„ | URL ìœ íš¨ì„± í™•ì¸ | ê³µì‹ í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ |
| 3ë‹¨ê³„ | ìºì‹œ | ë§ˆì§€ë§‰ ì„±ê³µ ë°ì´í„° |

**ì£¼ìš” ëŒ€ìƒ**: CISSP, CCNA, CCNP, OCA, OCP, PMP, CAPM

---

## âš™ï¸ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### `BaseScraper` (ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤)

ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ë¶€ëª¨ í´ë˜ìŠ¤ë¡œ, 3ë‹¨ê³„ Fallback ì „ëµì„ ê°•ì œí•©ë‹ˆë‹¤.

```python
class BaseScraper(ABC):
    source_name: str = "base"

    def fetch_schedules(self) -> List[Dict]:
        """3ë‹¨ê³„ Fallback ìˆœì°¨ ì‹¤í–‰"""
        # 1ë‹¨ê³„ â†’ 2ë‹¨ê³„ â†’ 3ë‹¨ê³„

    @abstractmethod
    def try_official_api(self) -> List[Dict]: ...
    @abstractmethod
    def try_web_scraping(self) -> List[Dict]: ...

    def save_to_db(self) -> Dict:
        """ìˆ˜ì§‘ ë°ì´í„°ë¥¼ DBì— Upsert"""

    @abstractmethod
    def close(self): ...
```

### DB í—¬í¼ í•¨ìˆ˜

| í•¨ìˆ˜ | ì„¤ëª… |
|------|------|
| `get_sync_engine()` | ë™ê¸° DB ì—”ì§„ ì‹±ê¸€í„´ (í¬ë¡¤ëŸ¬ìš©, `@lru_cache`) |
| `find_cert_id(session, name_ko)` | ìê²©ì¦ ì´ë¦„(ì •í™•ì¼ì¹˜)ìœ¼ë¡œ UUID ì¡°íšŒ |
| `find_cert_id_like(session, keyword)` | ìê²©ì¦ ì´ë¦„(ë¶€ë¶„ì¼ì¹˜ ILIKE)ìœ¼ë¡œ UUID ì¡°íšŒ |
| `upsert_schedule(...)` | ì‹œí—˜ ì¼ì • Upsert (`cert_id + round` ì¤‘ë³µ í™•ì¸) |
| `parse_date(date_str)` | ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹± |

### ìºì‹œ ìœ í‹¸ë¦¬í‹°

| í•¨ìˆ˜ | ì„¤ëª… |
|------|------|
| `save_cache(source, data)` | ì„±ê³µ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ |
| `load_cache(source)` | ë§ˆì§€ë§‰ ì„±ê³µ ìºì‹œ JSON ë¡œë“œ |

**ìºì‹œ íŒŒì¼ í˜•ì‹**:
```json
{
  "fetched_at": "2026-02-09T03:01:30",
  "source": "qnet",
  "count": 12,
  "schedules": [
    {
      "cert_name": "ì •ë³´ì²˜ë¦¬ê¸°ì‚¬",
      "round": 1,
      "reg_start": "2026-01-13",
      "reg_end": "2026-01-16",
      "exam_date": "2026-02-22",
      "result_date": "2026-03-20"
    }
  ]
}
```

---

## ğŸ”„ Upsert (Conflict Resolution) ì „ëµ

í¬ë¡¤ëŸ¬ì˜ `upsert_schedule` í•¨ìˆ˜ëŠ” ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¦…ë‹ˆë‹¤:

1. **`cert_id` + `round`** ì¡°í•©ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
2. **ê¸°ì¡´ ë°ì´í„° ìˆìœ¼ë©´** â†’ `COALESCE`ë¡œ NULLì´ ì•„ë‹Œ ê°’ë§Œ ì—…ë°ì´íŠ¸ + `updated_at` ê°±ì‹ 
3. **ê¸°ì¡´ ë°ì´í„° ì—†ìœ¼ë©´** â†’ ìƒˆë¡œ INSERT

```sql
-- ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸ (NULLì´ ì•„ë‹Œ ê°’ë§Œ)
UPDATE exam_schedules
SET reg_start = COALESCE(:rs, reg_start),
    reg_end = COALESCE(:re, reg_end),
    exam_date = COALESCE(:ed, exam_date),
    result_date = COALESCE(:rd, result_date),
    updated_at = NOW()
WHERE cert_id = :cid AND round = :r
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### CLI ì§ì ‘ ì‹¤í–‰

```bash
cd backend

# ì „ì²´ í¬ë¡¤ëŸ¬ ì‹¤í–‰
python -m crawlers.run_crawlers

# ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰
python -m crawlers.run_crawlers --qnet        # Q-Netë§Œ
python -m crawlers.run_crawlers --kdata       # KDataë§Œ
python -m crawlers.run_crawlers --cloud       # Cloudë§Œ
python -m crawlers.run_crawlers --finance     # ê¸ˆìœµë§Œ
python -m crawlers.run_crawlers --itdomestic  # êµ­ë‚´ ITë§Œ
python -m crawlers.run_crawlers --intl        # êµ­ì œ CBTë§Œ
```

### APIë¥¼ í†µí•œ ìˆ˜ë™ ì‹¤í–‰

```bash
# ì „ì²´ í¬ë¡¤ë§
curl -X POST http://localhost:8000/api/crawl/trigger

# íŠ¹ì • ì†ŒìŠ¤ë§Œ
curl -X POST "http://localhost:8000/api/crawl/trigger?source=qnet"
```

### ìë™ ìŠ¤ì¼€ì¤„ë§ (APScheduler)

`backend/services/scheduler.py`ì—ì„œ APSchedulerë¡œ ì •ê¸° ì‹¤í–‰ì„ ê´€ë¦¬í•©ë‹ˆë‹¤:

- **ì‹¤í–‰ ì£¼ê¸°**: ë§¤ì¼ ìƒˆë²½ 3ì‹œ (KST)
- **ì‹¤í–‰ ë‚´ìš©**: ì „ì²´ í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰ â†’ seed-events.ts ë™ê¸°í™”
- **ì—ëŸ¬ ì²˜ë¦¬**: ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ í¬ë¡¤ëŸ¬ëŠ” ê³„ì† ì‹¤í–‰

```python
# scheduler.py ì—ì„œì˜ ì„¤ì • (ì˜ˆì‹œ)
scheduler.add_job(
    run_crawl_job,
    trigger=CronTrigger(hour=3, minute=0, timezone="Asia/Seoul"),
    id="daily_crawl",
    name="ì¼ì¼ í¬ë¡¤ë§",
    replace_existing=True,
)
```

---

## ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ ì¶œë ¥ í˜•ì‹

```
============================================================
ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ ìš”ì•½ - 2026-02-09 03:05:30
============================================================
  âœ… Q-Net: success (12.5s) â€” ğŸŸ¢ ê³µì‹ API
       ë§¤ì¹­: 12, ì‹ ê·œ: 3, ì—…ë°ì´íŠ¸: 5, ê±´ë„ˆëœ€: 0
  âœ… KData: success (5.3s) â€” ğŸŸ¡ ì›¹ í¬ë¡¤ë§
       ë§¤ì¹­: 4, ì‹ ê·œ: 0, ì—…ë°ì´íŠ¸: 4, ê±´ë„ˆëœ€: 0
  âœ… Cloud: success (8.1s) â€” ğŸŸ  ìºì‹œ ë°ì´í„°
       ë§¤ì¹­: 12, ì—…ë°ì´íŠ¸: 10, ê±´ë„ˆëœ€: 2
  âŒ Finance: failed (2.0s) â€” ğŸ”´ ì‹¤íŒ¨
       ì—ëŸ¬: Connection timeout
  âœ… IT Domestic: success (6.2s) â€” ğŸŸ¢ ê³µì‹ API
       ë§¤ì¹­: 8, ì‹ ê·œ: 1, ì—…ë°ì´íŠ¸: 7, ê±´ë„ˆëœ€: 0
  âœ… Intl Cert: success (4.5s) â€” ğŸŸ¡ ì›¹ í¬ë¡¤ë§
       ë§¤ì¹­: 15, ì—…ë°ì´íŠ¸: 15, ê±´ë„ˆëœ€: 0
------------------------------------------------------------
  ğŸ“ˆ í•©ê³„ â€” ì‹ ê·œ: 4, ì—…ë°ì´íŠ¸: 41, ê±´ë„ˆëœ€: 2
============================================================
```

### ìˆ˜ì§‘ ë°©ë²• ë¼ë²¨

| ë¼ë²¨ | ì˜ë¯¸ |
|------|------|
| ğŸŸ¢ ê³µì‹ API | 1ë‹¨ê³„ API ì„±ê³µ |
| ğŸŸ¡ ì›¹ í¬ë¡¤ë§ | 2ë‹¨ê³„ í¬ë¡¤ë§ ì„±ê³µ |
| ğŸŸ  ìºì‹œ ë°ì´í„° | 3ë‹¨ê³„ ìºì‹œ ì‚¬ìš© |
| ğŸ”´ ì‹¤íŒ¨ | ëª¨ë“  ë‹¨ê³„ ì‹¤íŒ¨ |
| âšª ë¯¸ì‹¤í–‰ | ì‹¤í–‰ë˜ì§€ ì•ŠìŒ |

### APIë¡œ ìƒíƒœ í™•ì¸

```bash
# í¬ë¡¤ë§ ìƒíƒœ ìš”ì•½
curl http://localhost:8000/api/crawl/status

# ìµœê·¼ ì´ë ¥ ì¡°íšŒ
curl http://localhost:8000/api/crawl/logs?limit=10

# í†µê³„ ìš”ì•½
curl http://localhost:8000/api/crawl/stats
```

---

## ğŸ”§ seed-events.ts ìë™ ë™ê¸°í™”

í¬ë¡¤ë§ ì™„ë£Œ í›„, DBì˜ ì‹œí—˜ ì¼ì • ë°ì´í„°ë¥¼ `frontend/lib/seed-events.ts` íŒŒì¼ë¡œ ìë™ ë™ê¸°í™”í•©ë‹ˆë‹¤.

**ëª©ì **: ë°±ì—”ë“œ APIê°€ ë‹¤ìš´ë˜ì–´ë„ í”„ë¡ íŠ¸ì—”ë“œê°€ ì •ì  ë°ì´í„°ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆë„ë¡ ë³´ì¥

**ë™ê¸°í™” íë¦„**:
1. í¬ë¡¤ë§ ì™„ë£Œ
2. DBì—ì„œ ì „ì²´ ì‹œí—˜ ì¼ì • ì¡°íšŒ
3. FullCalendar ì´ë²¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
4. `frontend/lib/seed-events.ts`ì— TypeScript ë°°ì—´ë¡œ ê¸°ë¡

```bash
# ìˆ˜ë™ ë™ê¸°í™”
curl -X POST http://localhost:8000/api/crawl/sync-seed
```

---

## ğŸ› ï¸ ìƒˆ í¬ë¡¤ëŸ¬ ì¶”ê°€ ê°€ì´ë“œ

### 1. í¬ë¡¤ëŸ¬ íŒŒì¼ ìƒì„±

`backend/crawlers/` ì— ìƒˆ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

```python
# backend/crawlers/new_scraper.py

from crawlers.base import BaseScraper
from typing import List, Dict

class NewScraper(BaseScraper):
    source_name = "new_source"  # ê³ ìœ  ì´ë¦„

    def try_official_api(self) -> List[Dict]:
        """1ë‹¨ê³„: API í˜¸ì¶œ"""
        # êµ¬í˜„...
        return []

    def try_web_scraping(self) -> List[Dict]:
        """2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§"""
        # êµ¬í˜„...
        return []

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        pass
```

### 2. ë°˜í™˜ ë°ì´í„° í˜•ì‹

```python
{
    "cert_name": "ìê²©ì¦ êµ­ë¬¸ëª…",  # certifications.name_koì™€ ë§¤ì¹­
    "round": 1,                    # ì‹œí—˜ íšŒì°¨
    "reg_start": "2026-03-01",     # ì ‘ìˆ˜ ì‹œì‘ì¼
    "reg_end": "2026-03-15",       # ì ‘ìˆ˜ ë§ˆê°ì¼
    "exam_date": "2026-04-20",     # ì‹œí—˜ì¼
    "result_date": "2026-05-10",   # ë°œí‘œì¼
}
```

### 3. ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì— ë“±ë¡

`backend/crawlers/run_crawlers.py`ì— ì‹¤í–‰ í•¨ìˆ˜ ì¶”ê°€:

```python
def run_new_source():
    from crawlers.new_scraper import NewScraper
    scraper = NewScraper()
    try:
        stats = scraper.save_to_db()
        return {"name": "New Source", "status": "success", "stats": stats}
    except Exception as e:
        return {"name": "New Source", "status": "failed", "error": str(e)}
    finally:
        scraper.close()
```

### 4. DBì— ìê²©ì¦ ë§ˆìŠ¤í„° ë°ì´í„° ì¶”ê°€

`database/seed.sql`ì— ìƒˆ ìê²©ì¦ì„ ì¶”ê°€í•˜ì—¬ `find_cert_id()`ê°€ ë§¤ì¹­í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
backend/crawlers/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base.py                  # BaseScraper + DB í—¬í¼ + ìºì‹œ ìœ í‹¸
â”œâ”€â”€ qnet_scraper.py          # Q-Net (êµ­ê°€ê¸°ìˆ ìê²©)
â”œâ”€â”€ kdata_scraper.py         # KData (ë°ì´í„° ìê²©ì‹œí—˜)
â”œâ”€â”€ cloud_scraper.py         # Cloud (AWS/GCP/Azure)
â”œâ”€â”€ finance_scraper.py       # Finance (ê¸ˆìœµ ìê²©ì¦)
â”œâ”€â”€ it_domestic_scraper.py   # IT Domestic (êµ­ë‚´ IT)
â”œâ”€â”€ intl_cert_scraper.py     # Intl Cert (êµ­ì œ CBT)
â””â”€â”€ run_crawlers.py          # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (ì „ì²´ ì‹¤í–‰ + ìš”ì•½)
```

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

| ë¬¸ì„œ | ì„¤ëª… |
|------|------|
| [ì„œë¹„ìŠ¤ ê°œìš”](./service-overview.md) | ì„œë¹„ìŠ¤ ì†Œê°œ |
| [ì•„í‚¤í…ì²˜](./architecture.md) | ì‹œìŠ¤í…œ êµ¬ì¡° |
| [API ëª…ì„¸](./api-reference.md) | í¬ë¡¤ë§ ê´€ë¦¬ API |
| [ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ](./database-schema.md) | í…Œì´ë¸” êµ¬ì¡° |
